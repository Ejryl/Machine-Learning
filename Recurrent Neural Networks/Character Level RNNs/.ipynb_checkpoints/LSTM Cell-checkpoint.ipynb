{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = torch.randn((2, 5, 3))\n",
    "input_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_cell_0 = nn.LSTMCell(hidden_size=16, input_size=3)\n",
    "lstm_cell_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hx = torch.zeros(5, 16)\n",
    "cx = torch.zeros(5, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1, c1 = lstm_cell_0(input_[0], (hx, cx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_cell_1 = nn.LSTMCell(input_size=3, hidden_size=h1.size()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_cell_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2, c2 = lstm_cell_1(input_[1], (h1, c1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTMCell in a module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a single layer LSTM with 128 cells and 8 hidden neurons each for the imput size of 5 features\n",
    "lstm = nn.ModuleList([nn.LSTMCell(input_size=5, hidden_size=8) for _ in range(128)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in lstm[0].named_parameters():\n",
    "    if name == 'bias_hh':\n",
    "        print(name, param)\n",
    "        nn.init.constant_(param, 0.0)\n",
    "        print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((20, 128, 5))  # init the data\n",
    "print(x.size())  # batch of 20 sequences, 10 characters long each, encoded in vectors of length 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the hidden and the cell states\n",
    "h = torch.zeros(20, 8)\n",
    "c = torch.zeros(20, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:, 0, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t, cell in enumerate(lstm):\n",
    "    (h, c) = cell(x[:, t, :], (h, c))\n",
    "\n",
    "print(h.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTMCell char-to-char model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batching function\n",
    "def get_batches(arr, n_seqs_in_a_batch, n_characters):\n",
    "    '''Create a generator that returns batches of size\n",
    "       n_seqs x n_steps from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       n_seqs: Batch size, the number of sequences per batch\n",
    "       n_steps: Number of sequence steps per batch\n",
    "    '''\n",
    "    \n",
    "    batch_size = n_seqs_in_a_batch * n_characters\n",
    "    n_batches = len(arr)//batch_size\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size]\n",
    "    # Reshape into n_seqs rows\n",
    "    arr = arr.reshape((n_seqs_in_a_batch, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_characters):\n",
    "        # The features\n",
    "        x = arr[:, n:n+n_characters]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+n_characters]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and prepare the data\n",
    "with open('./The Outcasts.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = tuple(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "int2char = dict(enumerate(characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2int = {char: index for index, char in int2char.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = np.array([char2int[char] for char in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "class CharLSTM(nn.ModuleList):\n",
    "    def __init__(self, sequence_len, vocab_size, hidden_dim, batch_size):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        \n",
    "        # init the meta parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_len = sequence_len\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # first lstm layer\n",
    "        self.lstm_1 = nn.LSTMCell(input_size=vocab_size, hidden_size=hidden_dim)\n",
    "        \n",
    "        # second lstm layer\n",
    "        self.lstm_2 = nn.LSTMCell(input_size=hidden_dim, hidden_size=hidden_dim) \n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "        # fully connected layer\n",
    "        self.fc = nn.Linear(in_features=hidden_dim, out_features=vocab_size)\n",
    "        \n",
    "    def forward(self, x, hc):\n",
    "        \"\"\"\n",
    "            x: input to the model\n",
    "                *  x[t] - input of shape (batch, input_size) at time t\n",
    "                \n",
    "            hc: hidden and cell states\n",
    "                *  tuple of hidden and cell state\n",
    "        \"\"\" \n",
    "        # empty tensor for the output of the lstm\n",
    "        output_seq = torch.empty((self.sequence_len, \n",
    "                                  self.batch_size, \n",
    "                                  self.vocab_size))\n",
    "        \n",
    "        # pass the hidden and the cell state from one lstm cell to the next one\n",
    "        # we also feed input at time step t to the cell\n",
    "        # init the both layer cells with the zero hidden and zero cell states\n",
    "        hc_1, hc_2 = hc, hc\n",
    "        \n",
    "        # for every step in the sequence\n",
    "        for t in range(self.sequence_len):\n",
    "            \n",
    "            # get the hidden and cell states from the first layer cell\n",
    "            hc_1 = self.lstm_1(x[t], hc_1)\n",
    "            \n",
    "            # unpack the hidden and the cell states from the first layer\n",
    "            h_1, c_1 = hc_1\n",
    "        \n",
    "            # pass the hidden state from the first layer to the cell in the second layer\n",
    "            hc_2 = self.lstm_2(h_1, hc_2)\n",
    "            \n",
    "            # unpack the hidden and cell states from the second layer cell\n",
    "            h_2, c_2 = hc_2\n",
    "        \n",
    "            # form the output of the fc\n",
    "            output_seq[t] = self.fc(self.dropout(h_2))\n",
    "        \n",
    "        # return the output sequence\n",
    "        return output_seq.view((self.sequence_len * self.batch_size, -1))\n",
    "          \n",
    "    def init_hidden(self):\n",
    "        \n",
    "        # initialize the hidden state and the cell state to zeros\n",
    "        return (torch.zeros(self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.batch_size, self.hidden_dim))\n",
    "    \n",
    "    def init_hidden_predict(self):\n",
    "        \n",
    "        # initialize the hidden state and the cell state to zeros\n",
    "        # batch size is 1\n",
    "        return (torch.zeros(1, self.hidden_dim),\n",
    "                torch.zeros(1, self.hidden_dim))\n",
    "    \n",
    "    def predict(self, char, top_k=5, seq_len=128):\n",
    "        ''' Given a character, predict the next character.\n",
    "        \n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # placeholder for the generated text\n",
    "        seq = np.empty(seq_len+1)\n",
    "        seq[0] = char2int[char]\n",
    "        \n",
    "        # initialize the hidden and cell states\n",
    "        hc = self.init_hidden_predict()\n",
    "        \n",
    "        # now we need to encode the character - (1, vocab_size)\n",
    "        char = to_categorical(char2int[char], num_classes=self.vocab_size)\n",
    "        \n",
    "        # add the batch dimension\n",
    "        char = torch.from_numpy(char).unsqueeze(0)\n",
    "        \n",
    "        # now we need to pass the character to the first LSTM cell to obtain \n",
    "        # the predictions on the second character\n",
    "        hc_1, hc_2 = hc, hc\n",
    "        \n",
    "        # for the sequence length\n",
    "        for t in range(seq_len):\n",
    "            \n",
    "            # get the hidden and cell states from the first LSTM layer\n",
    "            hc_1 = self.lstm_1(char, hc_1)\n",
    "            h_1, _ = hc_1\n",
    "            \n",
    "            # get the hidden and cell states from the second LSTM layer\n",
    "            hc_2 = self.lstm_2(h_1, hc_2)\n",
    "            h_2, _ = hc_2            \n",
    "            \n",
    "            # pass the output of the cell through fully connected layer\n",
    "            h_2 = self.fc(h_2)\n",
    "            \n",
    "            # apply the softmax to the output to get the probabilities of the characters\n",
    "            h_2 = F.softmax(h_2, dim=1)\n",
    "            \n",
    "            # h_2 now holds the vector of predictions (1, vocab_size)\n",
    "            # we want to sample 5 top characters\n",
    "            p, top_char = h_2.topk(top_k)\n",
    "            \n",
    "            # get the top k characters by their probabilities\n",
    "            top_char = top_char.squeeze().numpy()\n",
    "            \n",
    "            # sample a character using its probability\n",
    "            p = p.detach().squeeze().numpy()\n",
    "            char = np.random.choice(top_char, p = p/p.sum())\n",
    "        \n",
    "            # append the character to the output sequence\n",
    "            seq[t+1] = char\n",
    "            \n",
    "            # prepare the character to be fed to the next LSTM cell\n",
    "            char = to_categorical(char, num_classes=self.vocab_size)\n",
    "            char = torch.from_numpy(char).unsqueeze(0)\n",
    "            \n",
    "        return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the network - sequence_len, vocab_size, hidden_dim, batch_size\n",
    "net = CharLSTM(sequence_len=128, vocab_size=len(char2int), hidden_dim=512, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss and the optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Train Loss: 4.379609, Validation Loss: 4.327936\n",
      "Epoch: 0, Batch: 10, Train Loss: 3.205886, Validation Loss: 3.243956\n",
      "Epoch: 0, Batch: 20, Train Loss: 3.164618, Validation Loss: 3.204849\n",
      "Epoch: 0, Batch: 30, Train Loss: 3.140395, Validation Loss: 3.189654\n",
      "Epoch: 0, Batch: 40, Train Loss: 3.133486, Validation Loss: 3.177029\n",
      "Epoch: 0, Batch: 50, Train Loss: 3.130346, Validation Loss: 3.171665\n",
      "Epoch: 0, Batch: 60, Train Loss: 3.129068, Validation Loss: 3.160579\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-444b1694f50c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# calculate the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# update the parameters of the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# get the validation and the training data\n",
    "val_idx = int(len(encoded) * (1 - 0.1))\n",
    "data, val_data = encoded[:val_idx], encoded[val_idx:]\n",
    "\n",
    "# empty list for the validation losses\n",
    "val_losses = list()\n",
    "samples = list()\n",
    "\n",
    "for epoch in range(10):\n",
    "    \n",
    "    # reinit the hidden and cell steates\n",
    "    hc = net.init_hidden()\n",
    "    \n",
    "    for i, (x, y) in enumerate(get_batches(data, 128, 128)):\n",
    "        \n",
    "        # get the torch tensors from the one-hot of training data\n",
    "        # also transpose the axis for the training set and the targets\n",
    "        x_train = torch.from_numpy(to_categorical(x, num_classes=net.vocab_size).transpose([1, 0, 2]))\n",
    "        targets = torch.from_numpy(y.T).type(torch.LongTensor)  # tensor of the target\n",
    "        \n",
    "        # zero out the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # get the output sequence from the input and the initial hidden and cell states\n",
    "        output = net(x_train, hc)\n",
    "    \n",
    "        # calculate the loss\n",
    "        # we need to calculate the loss across all batches, so we have to flat the targets tensor\n",
    "        loss = criterion(output, targets.contiguous().view(128*128))\n",
    "        \n",
    "        # calculate the gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # update the parameters of the model\n",
    "        optimizer.step()\n",
    "    \n",
    "        # feedback every 10 batches\n",
    "        if i % 10 == 0: \n",
    "            \n",
    "            # initialize the validation hidden state and cell state\n",
    "            val_h, val_c = net.init_hidden()\n",
    "            \n",
    "            for val_x, val_y in get_batches(val_data, 128, 128):\n",
    "                \n",
    "                # prepare the validation inputs and targets\n",
    "                val_x = torch.from_numpy(to_categorical(val_x).transpose([1, 0, 2]))\n",
    "                val_y = torch.from_numpy(val_y.T).type(torch.LongTensor).contiguous().view(128*128)\n",
    "            \n",
    "                # get the validation output\n",
    "                val_output = net(val_x, (val_h, val_c))\n",
    "                \n",
    "                # get the validation loss\n",
    "                val_loss = criterion(val_output, val_y)\n",
    "                \n",
    "                # append the validation loss\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "                # sample 256 chars\n",
    "                samples.append(''.join([int2char[int_] for int_ in net.predict(\"A\", seq_len=1024)]))\n",
    "                \n",
    "            print(\"Epoch: {}, Batch: {}, Train Loss: {:.6f}, Validation Loss: {:.6f}\".format(epoch, i, loss.item(), val_loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling code - UDACITY CODE\n",
    "def sample(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    net.eval()\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = net.predict(ch, h, cuda=False, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = net.predict(chars[-1], h, cuda=False, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample(net, 2000, prime='Anna', top_k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
